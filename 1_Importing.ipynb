{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic importing of modules, clever loop for importing all raw data frames efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Controls\\\\P110061.xlsx', 'Controls\\\\P110090.xlsx', 'Controls\\\\P110096.xlsx', 'Controls\\\\P110097.xlsx', 'Controls\\\\P110098.xlsx', 'Controls\\\\P110099.xlsx', 'Controls\\\\P110101.xlsx', 'Controls\\\\P110102.xlsx', 'Controls\\\\P110103.xlsx', 'Controls\\\\P110104.xlsx', 'Controls\\\\P110106.xlsx', 'Controls\\\\P110107.xlsx', 'Controls\\\\P110109.xlsx', 'Controls\\\\P110110.xlsx', 'Controls\\\\P110112.xlsx', 'Controls\\\\P110114.xlsx', 'Controls\\\\P110115.xlsx', 'Controls\\\\P110126_P110126_CPET.xlsx', 'Controls\\\\P110132_P110132_CPET.xlsx', 'Controls\\\\P110139_P110139_CPET.xlsx', 'Controls\\\\P110140_110140_CPET.xlsx', 'Controls\\\\P110148_P110148_CPET.xlsx', 'Controls\\\\P110149_P110149_CPET.xlsx', 'Controls\\\\P110150_P110150_CPET.xlsx', 'Controls\\\\P110151_P110151_CPET.xlsx', 'Controls\\\\P110152_P110152_CPET.xlsx']\n",
      "['LC\\\\P110017.xlsx', 'LC\\\\P110039.xlsx', 'LC\\\\P110050.xlsx', 'LC\\\\P110051.xlsx', 'LC\\\\P110055.xlsx', 'LC\\\\P110056.xlsx', 'LC\\\\P110058.xlsx', 'LC\\\\P110064.xlsx', 'LC\\\\P110085.xlsx', 'LC\\\\P110086.xlsx', 'LC\\\\P110087.xlsx']\n",
      "['ME\\\\P110116_P110116_CPET.xlsx', 'ME\\\\P110117_CPET.xlsx', 'ME\\\\P110118_P110118_CPET.xlsx', 'ME\\\\P110119 _Braeden_CPET.xlsx', 'ME\\\\P110120_P110120_CPET.xlsx', 'ME\\\\P110121_110121_CPET.xlsx', 'ME\\\\P110122_P110122_CPET.xlsx', 'ME\\\\P110123_110123_CPET.xlsx', 'ME\\\\P110124_P110124_CPET.xlsx', 'ME\\\\P110127_P110127_CPET.xlsx', 'ME\\\\P110128_P110128_CPET.xlsx', 'ME\\\\P110129_110129_CPET.xlsx', 'ME\\\\P110130_P110130_CPET.xlsx', 'ME\\\\P110131_P110131_CPET.xlsx', 'ME\\\\P110133_P110133_CPET.xlsx', 'ME\\\\P110134_P1101334_CPET.xlsx', 'ME\\\\P110135_P110135_CPET.xlsx', 'ME\\\\P110137_P110137_CPET.xlsx', 'ME\\\\P110138_P110138_CPET.xlsx', 'ME\\\\P110141_P110141_CPET.xlsx', 'ME\\\\P110142_P110142_CPET.xlsx', 'ME\\\\P110143_P110143_CPET.xlsx', 'ME\\\\P110144_P110144_CPET.xlsx', 'ME\\\\P110145_P110145_CPET.xlsx', 'ME\\\\P110146_P110146_CPET.xlsx', 'ME\\\\P110147_P110147_CPET.xlsx']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_paths = [\"Controls\", \"LC\", \"ME\"]\n",
    "\n",
    "# Dictionaries to hold DataFrames and headers for each folder\n",
    "all_dfs = {}\n",
    "all_headers = {}\n",
    "\n",
    "for path in folder_paths:\n",
    "    csv_files = glob.glob(os.path.join(path, '*.xlsx'))\n",
    "    print(csv_files)\n",
    "    # Read data without headers, skipping the first 3 rows\n",
    "    dfs = [pd.read_excel(file, header=None, skiprows=3) for file in csv_files]\n",
    "    \n",
    "    # Read the header row separately (the row just before the data starts, so skiprows=2)\n",
    "    headers = [pd.read_excel(file, nrows=1, header=None) for file in csv_files]\n",
    "    \n",
    "    # Assign the header row to each DataFrame\n",
    "    for df, header in zip(dfs, headers):\n",
    "        df.columns = header.iloc[0]\n",
    "        \n",
    "    # Store the DataFrames and headers in the dictionaries\n",
    "    all_dfs[path] = dfs\n",
    "    all_headers[path] = headers\n",
    "\n",
    "# 25 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a copy of the grouped data frame to work with, to avoid having to import all the data frames over and over again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs_c = {'Controls':[], 'LC':[], 'ME':[]}\n",
    "groups = ['Controls', 'LC', 'ME']\n",
    "for group in groups:\n",
    "    group_list = []\n",
    "    for df in all_dfs[group]:\n",
    "        group_list.append(df.copy())\n",
    "    all_dfs_c[group] = group_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the amount of dataframes with missing columns_to_keep, and identify missing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t': 63, 'Power': 63, 'HR': 60, 'VE': 63, 'VO2': 63, 'VCO2': 63, 'PetCO2': 63, 'PetO2': 63, 'VO2/Kg': 63, 'VE/VO2': 63, 'VE/VCO2': 63, 'RQ': 63, 'VT': 63, 'Rf': 63, 'Ti': 63, 'Te': 63, 'Phase': 63}\n",
      "{'t': 0, 'Power': 0, 'HR': 3, 'VE': 0, 'VO2': 0, 'VCO2': 0, 'PetCO2': 0, 'PetO2': 0, 'VO2/Kg': 0, 'VE/VO2': 0, 'VE/VCO2': 0, 'RQ': 0, 'VT': 0, 'Rf': 0, 'Ti': 0, 'Te': 0, 'Phase': 0}\n",
      "HR\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "cols_to_use = ['t', 'Power', 'HR', 'VE', 'VO2', 'VCO2', 'PetCO2', 'PetO2', 'VO2/Kg', 'VE/VO2', 'VE/VCO2', 'RQ', 'VT', 'Rf', 'Ti', 'Te', 'Phase']\n",
    "count = {col: 0 for col in cols_to_use}\n",
    "no_count = {col: 0 for col in cols_to_use}\n",
    "for group_name, group in all_dfs_c.items():\n",
    "    for df in group:\n",
    "        for col in cols_to_use:\n",
    "            if col in df.columns:\n",
    "                count[col] += 1\n",
    "            else:\n",
    "                no_count[col] += 1\n",
    "print(count)\n",
    "print(no_count)\n",
    "for i in no_count:\n",
    "    if no_count[i] > 0:\n",
    "        print(i)\n",
    "        print(no_count[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unwanted columns, and subsequently removing NaN's   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group_name, group in all_dfs_c.items():\n",
    "    for df in group:\n",
    "        col_rem_2b = [col for col in df.columns if col not in cols_to_use]\n",
    "        df.drop(columns = col_rem_2b, inplace = True)\n",
    "\n",
    "for group_name, group in all_dfs_c.items():\n",
    "    for df in group:\n",
    "        df.dropna(inplace=True)\n",
    "Control = all_dfs_c['Controls']\n",
    "LC = all_dfs_c['LC']\n",
    "ME = all_dfs_c['ME']\n",
    "\n",
    "for groupname, group in all_dfs_c.items():\n",
    "    for i,df in enumerate(group):\n",
    "        df['Participant'] = f'{groupname}_{i}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat all dataframes of a single participant group, and change the column order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Control_df = pd.concat((Control), axis = 0)\n",
    "LC_df = pd.concat((LC), axis = 0)\n",
    "ME_df = pd.concat((ME), axis = 0)\n",
    "\n",
    "col_order = ['t', 'Power', 'HR', 'VE', 'VO2', 'VCO2', 'PetCO2', 'PetO2', 'VO2/Kg', 'VE/VO2', 'VE/VCO2', 'RQ', 'VT', 'Rf', 'Ti', 'Te', 'Participant', 'Phase']\n",
    "Control_df = Control_df[col_order]\n",
    "LC_df = LC_df[col_order]\n",
    "ME_df = ME_df[col_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHANGING DATE STRING TO SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hhmmss_to_seconds(hhmmss):\n",
    "    h, m, s = map(int, hhmmss.split(':'))\n",
    "    return h * 3600 + m * 60 + s\n",
    "\n",
    "for df in [Control_df, LC_df, ME_df]:\n",
    "    df['t'] = df['t'].astype('string')\n",
    "    df['t'] = df['t'].apply(hhmmss_to_seconds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows where the participant is not exercising (See paper for more information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "Control_df_edit = copy.deepcopy(Control_df)\n",
    "ME_df_edit = copy.deepcopy(ME_df)\n",
    "edit_c = dict(zip(['Controls_' + str(i) for i in range(21,26)], [190, 388, 331, 412, 290]))\n",
    "\n",
    "edit_ME = dict(zip(['ME_' + str(i) for i in range(22,26)], [318, 332, 350, 238]))\n",
    "Control_df_edit.reset_index(inplace = True)\n",
    "ME_df_edit.reset_index(inplace = True)\n",
    "for edit, df in zip([edit_c, edit_ME], [Control_df_edit, ME_df_edit]):\n",
    "    to_drop = []\n",
    "    for participant in edit.keys():\n",
    "        start_exercise = df.groupby('Participant').head(1).index.tolist()[int(participant[-2:])]\n",
    "        end_exercise = start_exercise + edit[participant]\n",
    "        end_trial = df.groupby('Participant').tail(1).index.tolist()[int(participant[-2:])]\n",
    "        to_drop += list(df.index[end_exercise:end_trial+1])\n",
    "    df.drop(to_drop, inplace = True)\n",
    "    df.set_index('index', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fixing Control Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Control_df_edit.reset_index(inplace = True)\n",
    "Control_df_edit.drop(columns = 'index', inplace = True)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fix HR \n",
    "Control_df_edit.loc[666:678, 'HR'] = Control_df_edit.loc[666:678, 'HR'].apply(lambda x: 84 if x < 50 else x)\n",
    "HR299 = Control_df_edit[Control_df_edit['HR'] < 50].index\n",
    "if HR299.size > 0:\n",
    "    Control_df_edit.loc[HR299[0], 'HR'] = np.mean([Control_df_edit['HR'][HR299[0]+1],Control_df_edit['HR'][HR299[0]-1]])\n",
    "\n",
    "# Fix VE/VO2\n",
    "def find_large_increments(list, increment):\n",
    "    result = []\n",
    "    for i in range(1, len(list)):\n",
    "        if list[i] - list[i-1] > increment:\n",
    "            result.append(i)\n",
    "    return result\n",
    "vevo2_incr = find_large_increments(Control_df_edit['VE/VO2'], 25)\n",
    "plt.plot(Control_df_edit['VE/VO2'])\n",
    "plt.scatter(vevo2_incr, Control_df_edit['VE/VO2'][vevo2_incr], color = 'red')\n",
    "plt.title('VE/VO2 before correction')\n",
    "for i in vevo2_incr:\n",
    "    Control_df_edit.loc[i, 'VE/VO2'] = Control_df_edit.loc[i-1, 'VE/VO2']\n",
    "plt.figure()\n",
    "plt.plot(Control_df_edit['VE/VO2'])\n",
    "plt.title('VE/VO2 after correction')\n",
    "\n",
    "# Fix VE/VCO2\n",
    "plt.figure()\n",
    "vevco2_incr = find_large_increments(Control_df_edit['VE/VCO2'], 25)\n",
    "plt.plot(Control_df_edit['VE/VCO2'])\n",
    "plt.scatter(vevco2_incr, Control_df_edit['VE/VCO2'][vevco2_incr], color = 'red')\n",
    "plt.title('VE/VCO2 before correction')\n",
    "for i in vevco2_incr:\n",
    "    Control_df_edit.loc[i, 'VE/VCO2'] = Control_df_edit.loc[i-1, 'VE/VCO2']\n",
    "plt.figure()\n",
    "plt.plot(Control_df_edit['VE/VCO2'])\n",
    "plt.title('VE/VCO2 after correction')\n",
    "\n",
    "# Fix Ti\n",
    "plt.figure()\n",
    "plt.plot(Control_df_edit['Ti'])\n",
    "ti_incr = find_large_increments(Control_df_edit['Ti'], 2.5)\n",
    "plt.scatter(ti_incr, Control_df_edit['Ti'][ti_incr], color = 'red')\n",
    "plt.title('Inspiratory Time (Ti) before correction')\n",
    "for i in ti_incr:\n",
    "    Control_df_edit.loc[i, 'Ti'] = Control_df_edit.loc[i-1, 'Ti']\n",
    "plt.figure()\n",
    "plt.plot(Control_df_edit['Ti'])\n",
    "plt.title('Inspiratory Time (Ti) after correction')\n",
    "\n",
    "# Fix Te\n",
    "te_incr = find_large_increments(Control_df_edit['Te'], 2.5)\n",
    "plt.figure()\n",
    "plt.plot(Control_df_edit['Te'])\n",
    "plt.scatter(te_incr, Control_df_edit['Te'][te_incr], color = 'red')\n",
    "plt.title('Expiratory Time (Te) before correction')\n",
    "for i in te_incr:\n",
    "    Control_df_edit.loc[i, 'Te'] = Control_df_edit.loc[i-1, 'Te']\n",
    "te_incr = find_large_increments(Control_df_edit['Te'], 4)\n",
    "for i in te_incr:\n",
    "    Control_df_edit.loc[i, 'Te'] = Control_df_edit.loc[i-1, 'Te']\n",
    "plt.figure()\n",
    "plt.plot(Control_df_edit['Te'])\n",
    "plt.title('Expiratory Time (Te) after correction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fixing ME data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ME_df_edit.reset_index(inplace = True)\n",
    "ME_df_edit.drop(columns = 'index', inplace = True)\n",
    "\n",
    "for i in ME_df_edit['Participant'].unique():\n",
    "    print(ME_df_edit.loc[:2700][(ME_df_edit.loc[:2700]['Participant'] == i) & (ME_df_edit.loc[:2700]['HR'] < 70)]['HR'])\n",
    "ME_df_edit['HR'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Replace values in 'HR' column that are less than 80 with NaN in the first 2700 rows\n",
    "ME_df_edit.loc[:2700, 'HR'] = ME_df_edit.loc[:2700, 'HR'].mask(ME_df_edit.loc[:2700, 'HR'] < 80, np.nan)\n",
    "\n",
    "vevo2_incr = find_large_increments(ME_df_edit['VE/VO2'], 30)\n",
    "plt.figure()\n",
    "plt.plot(ME_df_edit['VE/VO2'])\n",
    "plt.scatter(vevo2_incr, ME_df_edit['VE/VO2'][vevo2_incr], color = 'red')\n",
    "plt.title('VE/VO2 before correction')\n",
    "for i in vevo2_incr:\n",
    "    ME_df_edit.loc[i, 'VE/VO2'] = ME_df_edit.loc[i-1, 'VE/VO2']\n",
    "plt.figure()\n",
    "plt.plot(ME_df_edit['VE/VO2'])\n",
    "plt.title('VE/VO2 after correction')\n",
    "\n",
    "vevco2_incr = find_large_increments(ME_df_edit['VE/VCO2'], 28)\n",
    "plt.figure()\n",
    "plt.plot(ME_df_edit['VE/VCO2'])\n",
    "plt.scatter(vevco2_incr, ME_df_edit['VE/VCO2'][vevco2_incr], color = 'red')\n",
    "plt.title('VE/VCO2 before correction')\n",
    "for i in vevco2_incr:\n",
    "    ME_df_edit.loc[i, 'VE/VCO2'] = ME_df_edit.loc[i-1, 'VE/VCO2']\n",
    "plt.figure()\n",
    "plt.plot(ME_df_edit['VE/VCO2'])\n",
    "plt.title('VE/VCO2 after correction')\n",
    "\n",
    "ti_incr = find_large_increments(ME_df_edit['Ti'], 3)\n",
    "plt.figure()\n",
    "plt.plot(ME_df_edit['Ti'])\n",
    "plt.scatter(ti_incr, ME_df_edit['Ti'][ti_incr], color = 'red')\n",
    "plt.title('Inspiratory Time (Ti) before correction')\n",
    "for i in ti_incr:\n",
    "    ME_df_edit.loc[i, 'Ti'] = ME_df_edit.loc[i-1, 'Ti']\n",
    "plt.figure()\n",
    "plt.plot(ME_df_edit['Ti'])\n",
    "plt.title('Inspiratory Time (Ti) after correction')\n",
    "\n",
    "te_incr = find_large_increments(ME_df_edit['Te'], 3)\n",
    "plt.figure()\n",
    "plt.plot(ME_df_edit['Te'])\n",
    "plt.scatter(te_incr, ME_df_edit['Te'][te_incr], color = 'red')\n",
    "plt.title('Expiratory Time (Te) before correction')\n",
    "for i in te_incr:\n",
    "    ME_df_edit.loc[i, 'Te'] = ME_df_edit.loc[i-1, 'Te']\n",
    "plt.figure()\n",
    "plt.plot(ME_df_edit['Te'])\n",
    "plt.title('Expiratory Time (Te) after correction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LC_df_edit = copy.deepcopy(LC_df)\n",
    "LC_df_edit.reset_index(inplace = True)\n",
    "LC_df_edit.drop(columns = 'index', inplace = True)\n",
    "LC_df_edit['HR'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "vevo2_incr = find_large_increments(LC_df_edit['VE/VO2'], 30)\n",
    "plt.figure()\n",
    "plt.plot(LC_df_edit['VE/VO2'])\n",
    "plt.scatter(vevo2_incr, LC_df_edit['VE/VO2'][vevo2_incr], color = 'red')\n",
    "plt.title('VE/VO2 before correction')\n",
    "for i in vevo2_incr:\n",
    "    LC_df_edit.loc[i, 'VE/VO2'] = LC_df_edit.loc[i-1, 'VE/VO2']\n",
    "plt.figure()\n",
    "plt.plot(LC_df_edit['VE/VO2'])\n",
    "plt.title('VE/VO2 after correction')    \n",
    "\n",
    "vevco2_incr = find_large_increments(LC_df_edit['VE/VCO2'], 30)\n",
    "plt.figure()\n",
    "plt.plot(LC_df_edit['VE/VCO2'])\n",
    "plt.scatter(vevco2_incr, LC_df_edit['VE/VCO2'][vevco2_incr], color = 'red')\n",
    "plt.title('VE/VCO2 before correction')\n",
    "for i in vevco2_incr:\n",
    "    LC_df_edit.loc[i, 'VE/VCO2'] = LC_df_edit.loc[i-1, 'VE/VCO2']\n",
    "plt.figure()\n",
    "plt.plot(LC_df_edit['VE/VCO2'])\n",
    "plt.title('VE/VCO2 after correction')\n",
    "\n",
    "tv_incr = find_large_increments(LC_df_edit['VT'], 2)\n",
    "plt.figure()\n",
    "plt.plot(LC_df_edit['VT'])\n",
    "plt.scatter(tv_incr, LC_df_edit['VT'][tv_incr], color = 'red')\n",
    "plt.title('Tidal Volume (VT) before correction')\n",
    "for i in tv_incr:\n",
    "    LC_df_edit.loc[i, 'VT'] = LC_df_edit.loc[i-1, 'VT']\n",
    "tv_incra = find_large_increments(LC_df_edit['VT'], 2)\n",
    "plt.figure()\n",
    "plt.plot(LC_df_edit['VT'])\n",
    "plt.scatter(tv_incra, LC_df_edit['VT'][tv_incra], color = 'red')\n",
    "plt.title('Tidal Volume (VT) After 1st correction')\n",
    "for i in tv_incra:\n",
    "    LC_df_edit.loc[i, 'VT'] = LC_df_edit.loc[i-1, 'VT']\n",
    "plt.figure()\n",
    "plt.plot(LC_df_edit['VT'])\n",
    "plt.title('Tidal Volume (VT) After 2nd correction')\n",
    "\n",
    "\n",
    "ti_incr = find_large_increments(LC_df_edit['Ti'], 2.5)\n",
    "plt.figure()\n",
    "plt.plot(LC_df_edit['Ti'])\n",
    "plt.scatter(ti_incr, LC_df_edit['Ti'][ti_incr], color = 'red')\n",
    "plt.title('Inspiratory Time (Ti) before correction')\n",
    "for i in ti_incr:\n",
    "    LC_df_edit.loc[i, 'Ti'] = LC_df_edit.loc[i-1, 'Ti']\n",
    "plt.figure()\n",
    "plt.plot(LC_df_edit['Ti'])\n",
    "plt.title('Inspiratory Time (Ti) after correction')\n",
    "\n",
    "te_incr = find_large_increments(LC_df_edit['Te'], 2.5)\n",
    "plt.figure()\n",
    "plt.plot(LC_df_edit['Te'])\n",
    "plt.scatter(te_incr, LC_df_edit['Te'][te_incr], color = 'red')\n",
    "plt.title('Expiratory Time (Te) before correction')\n",
    "for i in te_incr:\n",
    "    LC_df_edit.loc[i, 'Te'] = LC_df_edit.loc[i-1, 'Te']\n",
    "plt.figure()\n",
    "te_incr = find_large_increments(LC_df_edit['Te'], 2.5)\n",
    "plt.plot(LC_df_edit['Te'])\n",
    "plt.scatter(te_incr, LC_df_edit['Te'][te_incr], color = 'red')\n",
    "plt.title('Expiratory Time (Te) after 1st correction')\n",
    "for i in te_incr:\n",
    "    LC_df_edit.loc[i, 'Te'] = LC_df_edit.loc[i-1, 'Te']\n",
    "plt.figure()\n",
    "plt.plot(LC_df_edit['Te'])\n",
    "plt.title('Expiratory Time (Te) after 2nd correction')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing participant 6 for being a huge outlier (TEMPORARY DECISION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idxdx = LC_df_edit['Participant'] == 'LC_6'\n",
    "#LC_df_edit = LC_df_edit.loc[-idxdx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tijn\\AppData\\Local\\Temp\\ipykernel_7208\\10016649.py:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'NAN' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('NAN', inplace = True)\n"
     ]
    }
   ],
   "source": [
    "for df in [Control_df_edit, LC_df_edit, ME_df_edit]:\n",
    "    df.fillna('NAN', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group_name, df_group in zip(['Controls', 'LC', 'ME'], [Control_df_edit, LC_df_edit, ME_df_edit]):     \n",
    "    df_group.to_csv(f'{group_name}.data', sep=',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def explory(dataframe, column):\n",
    "    plt.figure(1)\n",
    "    plt.plot(dataframe[column])\n",
    "    plt.title(column)\n",
    "    plt.show()\n",
    "    plt.figure(2)\n",
    "    plt.hist(dataframe[column])\n",
    "    plt.title(column)\n",
    "    plt.show()\n",
    "    plt.figure(3)\n",
    "    plt.boxplot(dataframe[column])\n",
    "    plt.title(column)\n",
    "    plt.show()\n",
    "    print(dataframe[column].describe())\n",
    "    print(dataframe[column].isna().sum())\n",
    "df = ME_df_edit\n",
    "df.reset_index(inplace = True)\n",
    "df.drop(columns = 'index', inplace = True)\n",
    "for column in df.columns:\n",
    "    explory(df, column)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
