{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic importing of modules, clever loop for importing all raw data frames efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Controls\\\\P110061.xlsx', 'Controls\\\\P110090.xlsx', 'Controls\\\\P110096.xlsx', 'Controls\\\\P110097.xlsx', 'Controls\\\\P110098.xlsx', 'Controls\\\\P110099.xlsx', 'Controls\\\\P110101.xlsx', 'Controls\\\\P110102.xlsx', 'Controls\\\\P110103.xlsx', 'Controls\\\\P110104.xlsx', 'Controls\\\\P110106.xlsx', 'Controls\\\\P110107.xlsx', 'Controls\\\\P110109.xlsx', 'Controls\\\\P110110.xlsx', 'Controls\\\\P110112.xlsx', 'Controls\\\\P110114.xlsx', 'Controls\\\\P110115.xlsx', 'Controls\\\\P110126_P110126_CPET.xlsx', 'Controls\\\\P110132_P110132_CPET.xlsx', 'Controls\\\\P110139_P110139_CPET.xlsx', 'Controls\\\\P110140_110140_CPET.xlsx', 'Controls\\\\P110148_P110148_CPET.xlsx', 'Controls\\\\P110149_P110149_CPET.xlsx', 'Controls\\\\P110150_P110150_CPET.xlsx', 'Controls\\\\P110151_P110151_CPET.xlsx', 'Controls\\\\P110152_P110152_CPET.xlsx']\n",
      "['LC\\\\P110017.xlsx', 'LC\\\\P110039.xlsx', 'LC\\\\P110050.xlsx', 'LC\\\\P110051.xlsx', 'LC\\\\P110055.xlsx', 'LC\\\\P110056.xlsx', 'LC\\\\P110058.xlsx', 'LC\\\\P110064.xlsx', 'LC\\\\P110085.xlsx', 'LC\\\\P110086.xlsx', 'LC\\\\P110087.xlsx']\n",
      "['ME\\\\P110116_P110116_CPET.xlsx', 'ME\\\\P110117_CPET.xlsx', 'ME\\\\P110118_P110118_CPET.xlsx', 'ME\\\\P110119 _Braeden_CPET.xlsx', 'ME\\\\P110120_P110120_CPET.xlsx', 'ME\\\\P110121_110121_CPET.xlsx', 'ME\\\\P110122_P110122_CPET.xlsx', 'ME\\\\P110123_110123_CPET.xlsx', 'ME\\\\P110124_P110124_CPET.xlsx', 'ME\\\\P110127_P110127_CPET.xlsx', 'ME\\\\P110128_P110128_CPET.xlsx', 'ME\\\\P110129_110129_CPET.xlsx', 'ME\\\\P110130_P110130_CPET.xlsx', 'ME\\\\P110131_P110131_CPET.xlsx', 'ME\\\\P110133_P110133_CPET.xlsx', 'ME\\\\P110134_P1101334_CPET.xlsx', 'ME\\\\P110135_P110135_CPET.xlsx', 'ME\\\\P110137_P110137_CPET.xlsx', 'ME\\\\P110138_P110138_CPET.xlsx', 'ME\\\\P110141_P110141_CPET.xlsx', 'ME\\\\P110142_P110142_CPET.xlsx', 'ME\\\\P110143_P110143_CPET.xlsx', 'ME\\\\P110144_P110144_CPET.xlsx', 'ME\\\\P110145_P110145_CPET.xlsx', 'ME\\\\P110146_P110146_CPET.xlsx', 'ME\\\\P110147_P110147_CPET.xlsx']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_paths = [\"Controls\", \"LC\", \"ME\"]\n",
    "\n",
    "# Dictionaries to hold DataFrames and headers for each folder\n",
    "all_dfs = {}\n",
    "all_headers = {}\n",
    "\n",
    "for path in folder_paths:\n",
    "    csv_files = glob.glob(os.path.join(path, '*.xlsx'))\n",
    "    print(csv_files)\n",
    "    # Read data without headers, skipping the first 3 rows\n",
    "    dfs = [pd.read_excel(file, header=None, skiprows=3) for file in csv_files]\n",
    "    \n",
    "    # Read the header row separately (the row just before the data starts, so skiprows=2)\n",
    "    headers = [pd.read_excel(file, nrows=1, header=None) for file in csv_files]\n",
    "    \n",
    "    # Assign the header row to each DataFrame\n",
    "    for df, header in zip(dfs, headers):\n",
    "        df.columns = header.iloc[0]\n",
    "        \n",
    "    # Store the DataFrames and headers in the dictionaries\n",
    "    all_dfs[path] = dfs\n",
    "    all_headers[path] = headers\n",
    "\n",
    "# 25 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a copy of the grouped data frame to work with, to avoid having to import all the data frames over and over again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs_c = {'Controls':[], 'LC':[], 'ME':[]}\n",
    "groups = ['Controls', 'LC', 'ME']\n",
    "for group in groups:\n",
    "    group_list = []\n",
    "    for df in all_dfs[group]:\n",
    "        group_list.append(df.copy())\n",
    "    all_dfs_c[group] = group_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the amount of dataframes with missing columns_to_keep, and identify missing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t': 63, 'Power': 63, 'HR': 60, 'VE': 63, 'VO2': 63, 'VCO2': 63, 'PetCO2': 63, 'PetO2': 63, 'VO2/Kg': 63, 'VE/VO2': 63, 'VE/VCO2': 63, 'RQ': 63, 'VT': 63, 'Rf': 63, 'Ti': 63, 'Te': 63, 'Phase': 63}\n",
      "{'t': 0, 'Power': 0, 'HR': 3, 'VE': 0, 'VO2': 0, 'VCO2': 0, 'PetCO2': 0, 'PetO2': 0, 'VO2/Kg': 0, 'VE/VO2': 0, 'VE/VCO2': 0, 'RQ': 0, 'VT': 0, 'Rf': 0, 'Ti': 0, 'Te': 0, 'Phase': 0}\n",
      "HR\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "cols_to_use = ['t', 'Power', 'HR', 'VE', 'VO2', 'VCO2', 'PetCO2', 'PetO2', 'VO2/Kg', 'VE/VO2', 'VE/VCO2', 'RQ', 'VT', 'Rf', 'Ti', 'Te', 'Phase']\n",
    "count = {col: 0 for col in cols_to_use}\n",
    "no_count = {col: 0 for col in cols_to_use}\n",
    "for group_name, group in all_dfs_c.items():\n",
    "    for df in group:\n",
    "        for col in cols_to_use:\n",
    "            if col in df.columns:\n",
    "                count[col] += 1\n",
    "            else:\n",
    "                no_count[col] += 1\n",
    "print(count)\n",
    "print(no_count)\n",
    "for i in no_count:\n",
    "    if no_count[i] > 0:\n",
    "        print(i)\n",
    "        print(no_count[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unwanted columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group_name, group in all_dfs_c.items():\n",
    "    for df in group:\n",
    "        col_rem_2b = [col for col in df.columns if col not in cols_to_use]\n",
    "        df.drop(columns = col_rem_2b, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    Identify NaN's per group, per dataframe, per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN checks for group 'Controls':\n",
      "DataFrame 0:\n",
      "DataFrame 1:\n",
      "DataFrame 2:\n",
      "  Power: 1 NaNs\n",
      "DataFrame 3:\n",
      "DataFrame 4:\n",
      "DataFrame 5:\n",
      "DataFrame 6:\n",
      "DataFrame 7:\n",
      "DataFrame 8:\n",
      "DataFrame 9:\n",
      "DataFrame 10:\n",
      "DataFrame 11:\n",
      "DataFrame 12:\n",
      "DataFrame 13:\n",
      "DataFrame 14:\n",
      "DataFrame 15:\n",
      "  HR: 1 NaNs\n",
      "DataFrame 16:\n",
      "DataFrame 17:\n",
      "  Power: 1 NaNs\n",
      "DataFrame 18:\n",
      "DataFrame 19:\n",
      "DataFrame 20:\n",
      "  HR: 1 NaNs\n",
      "DataFrame 21:\n",
      "DataFrame 22:\n",
      "DataFrame 23:\n",
      "DataFrame 24:\n",
      "DataFrame 25:\n",
      "NaN checks for group 'LC':\n",
      "DataFrame 0:\n",
      "DataFrame 1:\n",
      "DataFrame 2:\n",
      "DataFrame 3:\n",
      "DataFrame 4:\n",
      "DataFrame 5:\n",
      "DataFrame 6:\n",
      "  HR: 1 NaNs\n",
      "DataFrame 7:\n",
      "DataFrame 8:\n",
      "DataFrame 9:\n",
      "DataFrame 10:\n",
      "NaN checks for group 'ME':\n",
      "DataFrame 0:\n",
      "DataFrame 1:\n",
      "DataFrame 2:\n",
      "DataFrame 3:\n",
      "DataFrame 4:\n",
      "DataFrame 5:\n",
      "DataFrame 6:\n",
      "DataFrame 7:\n",
      "DataFrame 8:\n",
      "  HR: 2 NaNs\n",
      "DataFrame 9:\n",
      "DataFrame 10:\n",
      "DataFrame 11:\n",
      "DataFrame 12:\n",
      "DataFrame 13:\n",
      "  Power: 1 NaNs\n",
      "DataFrame 14:\n",
      "DataFrame 15:\n",
      "DataFrame 16:\n",
      "DataFrame 17:\n",
      "DataFrame 18:\n",
      "DataFrame 19:\n",
      "  HR: 2 NaNs\n",
      "DataFrame 20:\n",
      "DataFrame 21:\n",
      "  HR: 2 NaNs\n",
      "DataFrame 22:\n",
      "DataFrame 23:\n",
      "DataFrame 24:\n",
      "  Power: 1 NaNs\n",
      "DataFrame 25:\n"
     ]
    }
   ],
   "source": [
    "# Loop through each group in all_dfs\n",
    "for group_name, group in all_dfs_c.items():    \n",
    "    # Check for NaNs and print the summary\n",
    "    nan_checks = {}\n",
    "    for i, df in enumerate(group):\n",
    "        nan_checks[i] = {col: df[col].isna().sum() for col in df.columns}\n",
    "\n",
    "    # Print the NaN checks\n",
    "    print(f\"NaN checks for group '{group_name}':\")\n",
    "    for df_index, cols in nan_checks.items():\n",
    "        print(f\"DataFrame {df_index}:\")\n",
    "        for col, nan_count in cols.items():\n",
    "            if nan_count > 0:\n",
    "                print(f\"  {col}: {nan_count} NaNs\")\n",
    "# 0.5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hhmmss_to_seconds(hhmmss):\n",
    "    h, m, s = map(int, hhmmss.split(':'))\n",
    "    return h * 3600 + m * 60 + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group_name, group in all_dfs_c.items():\n",
    "    for df in group:\n",
    "        df.dropna(inplace=True)\n",
    "Control = all_dfs_c['Controls']\n",
    "LC = all_dfs_c['LC']\n",
    "ME = all_dfs_c['ME']\n",
    "\n",
    "for groupname, group in all_dfs_c.items():\n",
    "    for i,df in enumerate(group):\n",
    "        df['Participant'] = f'{groupname}_{i}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Control_df = pd.concat((Control), axis = 0)\n",
    "LC_df = pd.concat((LC), axis = 0)\n",
    "ME_df = pd.concat((ME), axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "col_order = ['t', 'Power', 'HR', 'VE', 'VO2', 'VCO2', 'PetCO2', 'PetO2', 'VO2/Kg', 'VE/VO2', 'VE/VCO2', 'RQ', 'VT', 'Rf', 'Ti', 'Te', 'Participant', 'Phase']\n",
    "Control_df = Control_df[col_order]\n",
    "LC_df = LC_df[col_order]\n",
    "ME_df = ME_df[col_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHANGING DATE STRING TO SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [Control_df, LC_df, ME_df]:\n",
    "    df['t'] = df['t'].astype('string')\n",
    "    df['t'] = df['t'].apply(hhmmss_to_seconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group_name, df_group in zip(['Controls', 'LC', 'ME'], [Control_df, LC_df, ME_df]):     \n",
    "    df_group.to_csv(f'{group_name}.data', sep=' ', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "-Heart rate values need to be cleaned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
